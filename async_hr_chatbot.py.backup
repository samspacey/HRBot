"""
Async version of HR Chatbot functionality.

This module provides asynchronous implementations of the core
HR Chatbot functions for improved performance and concurrency.
"""

import asyncio
import aiofiles
import logging
from typing import List, Tuple, Optional, AsyncGenerator
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from langchain_community.document_loaders import PyPDFLoader
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_core.documents import Document

from config import get_config
from cache import get_query_cache, get_embedding_cache
from validation import get_validator, ValidationError
from chunking import create_chunker

config = get_config()
query_cache = get_query_cache()
embedding_cache = get_embedding_cache()
validator = get_validator()
logger = logging.getLogger(__name__)


class AsyncHRChatbot:
    """
    Asynchronous HR Chatbot for improved performance and concurrency.
    """
    
    def __init__(self, max_workers: int = 4):
        """
        Initialize the async chatbot.
        
        Args:
            max_workers: Maximum number of worker threads for CPU-bound tasks
        """
        self.max_workers = max_workers
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.chunker = create_chunker(strategy="smart")
        self.embeddings = OpenAIEmbeddings(
            model=config.embedding_model,
            openai_api_key=config.openai_api_key
        )
        self.llm = ChatOpenAI(
            model=config.llm_model,
            temperature=0,
            openai_api_key=config.openai_api_key
        )
    
    async def __aenter__(self):
        """Async context manager entry."""
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        self.executor.shutdown(wait=True)
    
    async def _load_pdf_async(self, pdf_path: str) -> List[Document]:
        """
        Asynchronously load a single PDF file.
        
        Args:
            pdf_path: Path to the PDF file
            
        Returns:
            List of documents from the PDF
        """
        def load_pdf():
            try:
                validator.validate_pdf_file(pdf_path)
                loader = PyPDFLoader(pdf_path)
                docs = loader.load()
                
                # Tag source in metadata
                for doc in docs:
                    doc.metadata["source"] = pdf_path
                
                return docs
            except Exception as e:
                logger.error(f"Error loading PDF {pdf_path}: {str(e)}\")\n                return []\n        \n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(self.executor, load_pdf)\n    \n    async def _chunk_documents_async(self, documents: List[Document]) -> List[Document]:\n        \"\"\"\n        Asynchronously chunk documents.\n        \n        Args:\n            documents: Documents to chunk\n            \n        Returns:\n            Chunked documents\n        \"\"\"\n        def chunk_docs():\n            try:\n                return self.chunker.chunk_documents(documents)\n            except Exception as e:\n                logger.error(f\"Error chunking documents: {str(e)}\")\n                return []\n        \n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(self.executor, chunk_docs)\n    \n    async def load_and_split_pdfs_async(\n        self,\n        folder_path: Optional[str] = None,\n        batch_size: int = 5\n    ) -> AsyncGenerator[List[Document], None]:\n        \"\"\"\n        Asynchronously load and split PDFs with batching.\n        \n        Args:\n            folder_path: Path to folder containing PDFs\n            batch_size: Number of PDFs to process concurrently\n            \n        Yields:\n            Batches of processed documents\n        \"\"\"\n        folder_path = folder_path or config.policies_folder\n        \n        try:\n            validated_folder = validator.validate_folder_path(folder_path)\n            folder_path = str(validated_folder)\n        except ValidationError as e:\n            logger.error(f\"Invalid folder path: {str(e)}\")\n            return\n        \n        # Get PDF files\n        pdf_files = [\n            f for f in Path(folder_path).iterdir()\n            if f.suffix.lower() in config.allowed_file_extensions\n        ]\n        \n        if not pdf_files:\n            logger.warning(f\"No PDF files found in {folder_path}\")\n            return\n        \n        logger.info(f\"Found {len(pdf_files)} PDF files to process\")\n        \n        # Process PDFs in batches\n        for i in range(0, len(pdf_files), batch_size):\n            batch_files = pdf_files[i:i + batch_size]\n            logger.info(f\"Processing batch {i//batch_size + 1}: {len(batch_files)} files\")\n            \n            # Load PDFs concurrently\n            load_tasks = [\n                self._load_pdf_async(str(pdf_file))\n                for pdf_file in batch_files\n            ]\n            \n            batch_docs = await asyncio.gather(*load_tasks, return_exceptions=True)\n            \n            # Filter out exceptions and flatten\n            all_docs = []\n            for docs in batch_docs:\n                if isinstance(docs, Exception):\n                    logger.error(f\"Error in batch processing: {str(docs)}\")\n                    continue\n                all_docs.extend(docs)\n            \n            if all_docs:\n                # Chunk documents asynchronously\n                chunked_docs = await self._chunk_documents_async(all_docs)\n                if chunked_docs:\n                    logger.info(f\"Processed {len(chunked_docs)} chunks from batch\")\n                    yield chunked_docs\n    \n    async def build_vectorstore_async(\n        self,\n        documents: List[Document],\n        index_path: Optional[str] = None\n    ) -> Optional[FAISS]:\n        \"\"\"\n        Asynchronously build a FAISS vector store.\n        \n        Args:\n            documents: Documents to embed\n            index_path: Path to save the index\n            \n        Returns:\n            FAISS vectorstore instance\n        \"\"\"\n        index_path = index_path or config.index_path\n        \n        if not documents:\n            logger.error(\"No documents provided for vectorstore creation\")\n            raise ValueError(\"Documents list cannot be empty\")\n        \n        def build_vectorstore():\n            try:\n                logger.info(f\"Creating embeddings for {len(documents)} documents\")\n                vectorstore = FAISS.from_documents(documents, self.embeddings)\n                \n                logger.info(f\"Saving vectorstore to {index_path}\")\n                vectorstore.save_local(index_path)\n                \n                logger.info(\"Vectorstore created and saved successfully\")\n                return vectorstore\n            except Exception as e:\n                logger.error(f\"Error building vectorstore: {str(e)}\")\n                raise\n        \n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(self.executor, build_vectorstore)\n    \n    async def load_vectorstore_async(\n        self,\n        index_path: Optional[str] = None\n    ) -> Optional[FAISS]:\n        \"\"\"\n        Asynchronously load a FAISS vector store.\n        \n        Args:\n            index_path: Path to the saved index\n            \n        Returns:\n            FAISS vectorstore instance\n        \"\"\"\n        index_path = index_path or config.index_path\n        \n        def load_vectorstore():\n            try:\n                if not Path(index_path).exists():\n                    logger.error(f\"Index path does not exist: {index_path}\")\n                    raise FileNotFoundError(f\"Index path does not exist: {index_path}\")\n                \n                logger.info(f\"Loading vectorstore from {index_path}\")\n                vectorstore = FAISS.load_local(\n                    index_path,\n                    self.embeddings,\n                    allow_dangerous_deserialization=True,\n                )\n                \n                logger.info(\"Vectorstore loaded successfully\")\n                return vectorstore\n            except Exception as e:\n                logger.error(f\"Error loading vectorstore: {str(e)}\")\n                raise\n        \n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(self.executor, load_vectorstore)\n    \n    async def answer_query_async(\n        self,\n        question: str,\n        vectorstore: FAISS,\n        k: Optional[int] = None\n    ) -> Tuple[str, List[Document]]:\n        \"\"\"\n        Asynchronously answer a query.\n        \n        Args:\n            question: User's question\n            vectorstore: FAISS vectorstore instance\n            k: Number of documents to retrieve\n            \n        Returns:\n            Tuple of (answer, source_documents)\n        \"\"\"\n        k = k or config.default_k\n        \n        try:\n            # Validate inputs\n            question = validator.validate_query(question)\n            k = validator.validate_k_value(k)\n            \n            if vectorstore is None:\n                logger.error(\"Vectorstore is None\")\n                raise ValueError(\"Vectorstore cannot be None\")\n            \n            logger.info(f\"Processing query: {question[:100]}...\")\n            \n            # Check cache first\n            cached_result = query_cache.get(question, config.llm_model, k)\n            if cached_result is not None:\n                logger.info(\"Returning cached result\")\n                return cached_result\n            \n            def process_query():\n                try:\n                    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n                    \n                    # Create prompt\n                    prompt = PromptTemplate(\n                        template=(\n                            \"You are a helpful HR assistant. Below are excerpts from the company's policies.\\n\\n\"\n                            \"Context:\\n{context}\\n\\n\"\n                            \"Question: {question}\\n\"\n                            \"Answer concisely based ONLY on the provided context.\\n\"\n                            \"If the information is not present, respond exactly with \\\"I don't know.\\\"\"\n                        ),\n                        input_variables=[\"context\", \"question\"],\n                    )\n                    \n                    # Build QA chain\n                    qa = RetrievalQA.from_chain_type(\n                        llm=self.llm,\n                        chain_type=\"stuff\",\n                        retriever=retriever,\n                        return_source_documents=True,\n                        chain_type_kwargs={\"prompt\": prompt},\n                    )\n                    \n                    # Process query\n                    result = qa.invoke(question)\n                    answer = result[\"result\"].strip()\n                    docs = result.get(\"source_documents\", [])\n                    \n                    return answer, docs\n                except Exception as e:\n                    logger.error(f\"Error in query processing: {str(e)}\")\n                    raise\n            \n            # Run query processing in executor\n            loop = asyncio.get_event_loop()\n            answer, docs = await loop.run_in_executor(self.executor, process_query)\n            \n            logger.info(f\"Query processed successfully, retrieved {len(docs)} documents\")\n            \n            # Cache the result\n            result = (answer, docs)\n            query_cache.set(question, config.llm_model, k, result)\n            \n            return result\n            \n        except Exception as e:\n            logger.error(f\"Error processing query: {str(e)}\")\n            raise\n    \n    async def process_multiple_queries_async(\n        self,\n        questions: List[str],\n        vectorstore: FAISS,\n        k: Optional[int] = None\n    ) -> List[Tuple[str, str, List[Document]]]:\n        \"\"\"\n        Process multiple queries concurrently.\n        \n        Args:\n            questions: List of questions to process\n            vectorstore: FAISS vectorstore instance\n            k: Number of documents to retrieve per query\n            \n        Returns:\n            List of tuples (question, answer, source_documents)\n        \"\"\"\n        logger.info(f\"Processing {len(questions)} queries concurrently\")\n        \n        # Create tasks for all queries\n        tasks = [\n            self.answer_query_async(question, vectorstore, k)\n            for question in questions\n        ]\n        \n        # Execute all queries concurrently\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        \n        # Process results\n        processed_results = []\n        for i, (question, result) in enumerate(zip(questions, results)):\n            if isinstance(result, Exception):\n                logger.error(f\"Error processing question '{question}': {str(result)}\")\n                processed_results.append((question, f\"Error: {str(result)}\", []))\n            else:\n                answer, docs = result\n                processed_results.append((question, answer, docs))\n        \n        return processed_results\n    \n    async def build_index_from_folder_async(\n        self,\n        folder_path: Optional[str] = None,\n        index_path: Optional[str] = None,\n        batch_size: int = 5\n    ) -> Optional[FAISS]:\n        \"\"\"\n        Build a complete index from a folder of PDFs asynchronously.\n        \n        Args:\n            folder_path: Path to folder containing PDFs\n            index_path: Path to save the index\n            batch_size: Number of PDFs to process concurrently\n            \n        Returns:\n            FAISS vectorstore instance\n        \"\"\"\n        all_documents = []\n        \n        # Process documents in batches\n        async for batch_docs in self.load_and_split_pdfs_async(folder_path, batch_size):\n            all_documents.extend(batch_docs)\n            logger.info(f\"Total documents processed so far: {len(all_documents)}\")\n        \n        if not all_documents:\n            logger.error(\"No documents were processed\")\n            return None\n        \n        # Build vectorstore\n        vectorstore = await self.build_vectorstore_async(all_documents, index_path)\n        return vectorstore\n\n\n# Convenience functions for async operations\nasync def load_and_split_pdfs_async(\n    folder_path: Optional[str] = None,\n    batch_size: int = 5\n) -> List[Document]:\n    \"\"\"\n    Async convenience function to load and split PDFs.\n    \n    Args:\n        folder_path: Path to folder containing PDFs\n        batch_size: Number of PDFs to process concurrently\n        \n    Returns:\n        List of all processed documents\n    \"\"\"\n    async with AsyncHRChatbot() as chatbot:\n        all_documents = []\n        async for batch_docs in chatbot.load_and_split_pdfs_async(folder_path, batch_size):\n            all_documents.extend(batch_docs)\n        return all_documents\n\n\nasync def build_vectorstore_async(\n    documents: List[Document],\n    index_path: Optional[str] = None\n) -> Optional[FAISS]:\n    \"\"\"\n    Async convenience function to build vectorstore.\n    \n    Args:\n        documents: Documents to embed\n        index_path: Path to save the index\n        \n    Returns:\n        FAISS vectorstore instance\n    \"\"\"\n    async with AsyncHRChatbot() as chatbot:\n        return await chatbot.build_vectorstore_async(documents, index_path)\n\n\nasync def answer_query_async(\n    question: str,\n    vectorstore: FAISS,\n    k: Optional[int] = None\n) -> Tuple[str, List[Document]]:\n    \"\"\"\n    Async convenience function to answer a query.\n    \n    Args:\n        question: User's question\n        vectorstore: FAISS vectorstore instance\n        k: Number of documents to retrieve\n        \n    Returns:\n        Tuple of (answer, source_documents)\n    \"\"\"\n    async with AsyncHRChatbot() as chatbot:\n        return await chatbot.answer_query_async(question, vectorstore, k)\n\n\n# Example usage\nasync def main():\n    \"\"\"\n    Example usage of async HR chatbot.\n    \"\"\"\n    async with AsyncHRChatbot() as chatbot:\n        # Build index from folder\n        print(\"Building index...\")\n        vectorstore = await chatbot.build_index_from_folder_async()\n        \n        if vectorstore:\n            # Answer multiple queries concurrently\n            questions = [\n                \"What is the vacation policy?\",\n                \"How do I report harassment?\",\n                \"What are the sick leave rules?\"\n            ]\n            \n            print(\"Processing queries...\")\n            results = await chatbot.process_multiple_queries_async(questions, vectorstore)\n            \n            for question, answer, docs in results:\n                print(f\"\\nQ: {question}\")\n                print(f\"A: {answer}\")\n                print(f\"Sources: {len(docs)} documents\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())